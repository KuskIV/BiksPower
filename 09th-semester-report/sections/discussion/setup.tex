\section{Setup}

In this section, different aspects of how the experimental setup was made will be discussed.

\paragraph*{Test cases based on time or iterations:} In our work, the test cases are executed based on time rather than runs. In the literature, it is common to use runs as seen in\cite[]{Pereira2017,Koedijk2022diff,Georgiou2020}. One work running test cases based on time is the work by Sestoft\cite[]{sestoft2013microbenchmarks}, where a duration of $0.25$s is chosen to avoid problems with the virtual machine and the clock resolution. Another argument for using time made by Sestoft\cite[]{sestoft2013microbenchmarks} is when considering test cases of different sizes, where 100 million runs might be too time-consuming for larger test cases, but not smaller test cases. In our work, we chose to use time because of E3, where the test cases have to run for one minute for E3 to detect the test case. A side effect of running based on time is that the different test cases will not run for an equal amount of iterations. This potential issue is addressed in our work by running Cochran's formula on all test cases, thereby ensuring all test cases have enough measurements. 

% \paragraph*{A C\# implementation of the framework:} In other works,  

% \paragraph*{SQL:}

\paragraph*{Test cases:} In our work, the test cases used were chosen based on what is generally used in research\cite[]{Koedijk2022diff, greenland2016statistical}. The argument for using such test cases is to make our work more comparable to ensure everything is implemented correctly. Questions can however be raised concerning how well such test cases represent a real-life application. Because of this, it could have been interesting to test the different measuring instruments on larger applications, which could be a potential future work. In our work, all test cases are implemented in C\#, as the focus is not to compare the energy consumption of any specific language, but rather to compare the measuring instruments. It could however have been interesting to test an even wider range of test cases, also including different languages. This could be interesting as when comparing the different measuring instruments in \cref{sec:iterations} some overall patterns could be observed, but there were always exceptions, like how RAPL only in some cases reported a higher energy consumption compared to IPG and LHM. Additional test cases could help to find patterns if, for example, RAPL was better/worse when measuring certain kinds of operations. This is however a task for future work.  
%% not unsafe
%% small test cases
%% only c#

\paragraph*{Background processes:} In our work, different processes were disabled, in order to limit their effect on the measurements. These processes include the ones presented in \cref{tab:disabled_proc} and Windows update. One thing to note here is how no background processes are disabled for Linux, as we were unable to find any processes which made sense to disable. For Windows, eight background processes were located and stopped upon startup for the DUT, but for both OSs, it can be argued more time could have been spent trying to limit the activity of background processes. This was based on the expectation that a fresh install of both OSs would mean they would have a limited amount of background processes, which is a subject for future work. 

\paragraph*{Temperature:} For the temperature, there was an expectation that when the CPU reached some temperature, an effect would be observable in the results. This did however not happen, as was discussed in \cref{subsec:TempBat}. The reason for this, as was also mentioned in \cref{subsec:TempBat}, was most likely due to the test cases never stressing the CPU enough to reach temperatures beyond 65 degrees celsius. As a result of this, no temperature limits were set when running the experiments. It could however have been interesting to see what impact temperatures above 65 degrees celsius had on the results. Given the temperatures never increased above 65 for the test cases used, a stress test could have been used for the experiment instead. The stress test would be expected to bring the CPU to higher temperatures, as it will bring the load of the CPU to 100\%, and since the effect of an overheated CPU would be expected to be similar for all test cases, it would be fine that the stress test is used instead of the test cases. This is however a subject for future work.

\paragraph*{Battery:} Given some issues on the surface devices as discussed in \cref{subsec:TempBat}, the battery limit were set between $40-80$\%. Within these limits, no visible effect of the battery limit could be found. The expectation for this experiment covered in \cref{subsec:TempBat} was to find some lower limit, where the energy consumption would change. This would occur as the DUT would enter a power saving mode when some battery percentage is reached. This power saving mode would most likely cause the OS to stop some background processes and underclock the CPU. When a CPU is underclocked, the test case would run fewer samples during a measurement. The energy consumption of one sample when the DUT is in power saving mode is expected to be different compared to when the DUT is not in power saving mode, but this is a subject for future work. For both surface devices, the power mode is set to maximum, where it could have been interesting to see how the results would change if the power saving mode would be different, this is however a subject for future works. 

%% power mode