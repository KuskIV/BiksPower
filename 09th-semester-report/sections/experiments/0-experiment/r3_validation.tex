\subsection{R3 Validation}

In this work, an adapted version of R3 validation is used, based on the work by Bokhari et al.\cite[]{Bokhari2020r3}. The data presented in this chapter is only based on three measuring instruments, as some problems occured while measureing using both E3 and the Clamp. When looking at the results for the different DUT's, similar observations can be made, which is why the results for only one of the DUT's can be observed in \cref{fig:PowerKomplett_HardwareMonitor_Cores_R3_dynamic_energy_without_outliers_Win32NT_avg_watts}, in this case the workstation. 


\input{tabels/experiment_results/R3/dynamic_energy/PowerKomplett/Win32NT/without_outliers/Cores/HardwareMonitor.tex}

\paragraph{Expectations:} R3 validation was chosen in this work, because Bokhari et al.\cite[]{Bokhari2020r3} observed the order of which the test cases were executed in, mattered. This was argued to be the case because the state of the DUT changed over time, where different background processes executed at different times, and would use different amount of processor power, and cause a garbage collection, which would impact the results. Because of this, it was deemed unfair to execute the test cases in the same order, which resulted in the R3 way of switching the order upon a restart. This was however done on an android phone, where this work in on windows and linux, where we expect less background processes to interfere with the results. Because of this, we do not expect R3 to have a major impact on the results.

\paragraph{Problems:} During the experiments of the impact of R3 validation, some of the profilers failed, and were thus excluded from the experiments. This was however concluded to be okay, as a conclusion could be made based only on iterating over Clamp, HardwareMonitor and Intel Power Gadget. Another issue occurred because of the problems with Intel Power Gadget, as covered in \cref{subsec:TempBat}, which caused the DUT to crash. The crashes occured especially for the Surface Book, less for the Surface Pro 4, and almost neve for the workstation, which is why the workstation was choosen as the DUT to illustrate in \cref{fig:PowerKomplett_HardwareMonitor_Cores_R3_dynamic_energy_without_outliers_Win32NT_avg_watts}. Because the Surface Book crashed, most of the restarts of this DUT would be a result of the background script, rather than the framework, which ment the distribution of which measurement instrument started, was not equal.

\paragraph{Conclusion:} When considering the results for the workstation, it can be seen in \cref{fig:PowerKomplett_HardwareMonitor_Cores_R3_dynamic_energy_without_outliers_Win32NT_avg_watts}. Here the energy consumption measured by the HardwareMonitor can be seen, where the measurements are the average dynamic energy in watts. On the y-axis, label describes what test case is measured, and which measuring instrument was the first to run. This means, for each test case, there are three plots, since three measuring instruments was used on the workstation on windows. What can be observed in \cref{fig:PowerKomplett_HardwareMonitor_Cores_R3_dynamic_energy_without_outliers_Win32NT_avg_watts} is a smaller impact than expected, where the R3 validation can be concluded to have no impact on the measurements at all. This means the assumption about less background processes in windows is true, at least on a fresh install of windows. Because of this, R3 validation will be disregarded, and not used anymore through the experiments. In addition to this, the E3 and Clamp experiments not executed yet, will be run independently, without iterating over all measuring instruments.
