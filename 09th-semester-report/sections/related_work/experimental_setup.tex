\section{Measuring Methodology and Setup}\label[section]{sec:rw_measureing_methodology_setup}

When running experiments on the energy consumption of hardware, many factors can impact the measurements. There are several existing works discussing the setup process and how to minimize such factors. One of which is covered in the work by Mancebo et al.\cite{GarciaFEETINGS} who found that there are three types of problems that occur in the domain of evaluating software's energy consumption.\cite{GarciaFEETINGS} 

\begin{enumerate}
    \item There are inconsistencies in the terminology, with different terms being used for the same concept or even the same term being used for different concepts. This lack of consistency hurts the understanding of the subject.
    \item There is no agreed-upon methodology, which makes it difficult to compare and replicate results.
    \item Choosing the correct measuring instruments that are fitting for the particular experiment evaluation.
\end{enumerate}

To solve these problems they created a framework called Framework for Energy Efficiency Testing to Improve Environmental Goals of the Software (FEETINGS). FEETINGS consists of three main components which aim to solve these issues. These are the conceptual component, methodological component, and technological component.\cite{GarciaFEETINGS}

\paragraph*{}
The conceptual component provides an extension of the work by García et al.\cite{GarciaSMO} where the Software Measurement Ontology is defined. This extension is called Green Software Measurement Ontology (GSMO) and provides terminology and the corresponding definitions to help authors of papers about energy measurements describe their process with a specified terminology. This also helps readers of papers about energy measurements understand and compare results from different papers. The Methodological component is a process called Green Software Measurement Process (GSMP) which are guidelines to assist with the study design, analysis, and presenting the results. It is presented in seven phases, described as follows by Mancebo et al.\cite{GarciaFEETINGS}

\begin{enumerate}
    \item \textit{"Scope Definition"}: A requirements specification for evaluation of the results is made and the test cases are chosen.
    \item \textit{"Measurement Environment Setting"}: The DUT and measuring instrument are chosen. The baseline measurements should also be acquired in this phase.
    \item \textit{"Measurement Environment Preparation"}: Preparation of environment
    \item \textit{"Perform the measurements"}: Experiment is conducted and data is recorded. 
    \item \textit{"Test Case Data analysis"}: The data is processed and analyzed. Here outliers can be found and a sanity check of the results is done.
    \item \textit{"Software Entity Data analysis"}: Conclusions about the experiments are started from the analysis in the previous step.
    \item \textit{"Reporting the results"}: Here the process and results of the study are documented. 
\end{enumerate} 

Mancebo et al.\cite{GarciaFEETINGS} claim that the methodological component aids in giving more reliable and consistent measurements which in turn makes them more comparable and replicable.

The technological component consists of two parts. The first part is the Energy Efficiency Tester (EET) which is a hardware-based approach to measuring energy consumption. It is presented in more detail by Mancebo et al. in \cite{GarciaEET}. EET is a portable measuring instrument, which has three main components: A system microcontroller which is a Mega Arduino Development Board responsible for getting the data. The Mega Arduino Development Board has a set of 9 sensors for measuring the energy consumption and temperature of different components of the DUT. Lastly, a power supply has to be used instead of the DUT's original power supply because the sensors are connected to the power supply.\cite{GarciaEET} The second part is called ELLIOT, which is a software tool that handles the data provided by EET with the main goal of providing a visual representation that can be used to process, analyze and construct graphs and tables of the data collected by the EET. It can identify potential outliers and calculate different statistical variables.\cite{GarciaFEETINGS} It should be noted that to the extent of our knowledge, the ELLIOT tool is not available to the public and can therefore not be used.\newline

Another work is by Sestoft\cite[]{sestoft2013microbenchmarks}, where a framework to measure the execution time of microbenchmarks is created in an iterative manner where different pitfalls are uncovered. The reason why energy consumption measurements are not straightforward is because of a large number of factors which has been introduced in complex modern systems. This is especially clear on managed execution platforms like the Common Language Infrastructure e.g .Net from Microsoft or the Java Virtual Machine (JVM) where software in an intermediate form is compiled to real machine code at runtime by just-in-time (JIT) compilation, which affects the execution time for a couple of reasons. One reason is the start-up overhead of the JIT or the adaptive optimization of JIT compilation. What the optimization does is that it estimates how many times different parts of the code are executed during run-time. This results in a prioritization, where a lot of time is used to generate optimized code for code executed many times, and code only executed a few times is quickly generated and thus less optimized. The JIT compiler also avoids using a lot of time on code analysis, which can result in cases where the generated code works well in simpler contexts, and in more complex contexts, the performance is not as good. Another factor the programmer cannot control is automatic memory management, which may decide to run the garbage collection during the run of a test case, resulting in unreliable results. The same can be said for the operating system, processors, and memory management systems, where garbage collectors can run, resulting in unreliable results.\newline

In the process of creating the framework for measuring execution time, Sestoft\cite[]{sestoft2013microbenchmarks} uses a \texttt{multiply} method which performs 20 floating point multiplications, an integer bitwise "and" and a conversion from a 32-bit int to a 64-bit double. During the first phases, one observation is that when running the test cases for many iterations, the execution time drops to zero. This happens because the JIT compiler observes that the result of the method is not used for anything, resulting in it skipping the method entirely. When measuring the runtime of the test case, only measuring one execution is deemed too little as the results vary too much. This should rather be multiple runs, and then looking at the average runtime and the standard deviation. These multiple runs are in the work by Sestoft et al.\cite{sestoft2013microbenchmarks} deemed to be until the execution time exceeds $0.25$ seconds, as this is long enough to avoid problems with virtual machine startup and clock resolution. In the end, some additional pitfalls are noted, including:\cite{sestoft2013microbenchmarks}

\begin{itemize}    
    \item Shut down as many background services as possible, as this can impact performance
    \item The generation of logging and debug messages can use more than 90\% of execution time, so this has to be disabled
    \item An IDE uses a lot of CPU time and causes slower execution time because of debugging code, so do not execute through IDEs
    \item Disable power-saving schemes so the CPU does not reduce its speed during benchmark runs.
    \item Compile with relevant optimization options so the generated bytecode does not include code to accommodate debugging
    \item Different implementations of .NET and JVM have different characteristics and different garbage collectors
    \item Different CPU brands, versions (like desktop or mobile), and hardware (like ram) have different characteristics
    \item Reflect on results, and if they look slow/fast something might have been overlooked.
\end{itemize}

%% benchmarking c# for energy consumption ørsted nielsen

\paragraph*{}
Bokhari et al\cite{Bokhari2020r3} found that when running benchmarks comparing different variants of the same program on Android systems, noise had an impact on the results. This was due to noise coming from several uncontrollable factors such as:\cite{Bokhari2020r3}

\begin{itemize}
    \item System software states changing from one restart to another and just changing during a run. This is partly because not all background processes from the system begin immediately after a reboot and additionally some are triggered by battery levels.
    \item Memory consumption of the Android system due to background processes that cannot be disabled and the Android memory management system.
    \item The battery voltage. Even though the system was fully charged at the starting point the voltage at the beginning and at the end of an experiment varied from run to run.
\end{itemize}

To solve this they propose a method called \textit{R3-VALIDATION}\cite{Bokhari2020r3} which is a rotated round-robin approach to running the test cases (program variants), which ensures more fair execution conditions. In this approach the test cases (A, B, C) are rotated as follows: setup, ABC, ABC, setup, BCA, BCA, setup, CAB, CAB. Here the setup phase is a restart, initialization, and recharge of the system. The amount of times to run a set of test cases between setup phases is based on the battery level. This battery level limit is chosen based on when the battery level would cause noise in the results, which occurs when the energy-saving mechanisms are started. They achieved more consistent system states and memory consumption from this approach and therefore gave more fair results for the different program variants.\cite{Bokhari2020r3}\nytafsnit

When comparing the results gathered from either different DUTs or different measuring instruments different steps need to be taken. In the work by Dongarra et al.\cite[]{Dongarra2012} the sampling rate is set to the same across all systems, to make the measurements more comparable.