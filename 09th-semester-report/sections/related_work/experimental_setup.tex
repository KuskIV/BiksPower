\section{Measuring methodology and setup}\label[section]{sec:rw_measureing_methodology_setup}

When running experiments on the energy consumption of hardware, there are many factors which can impact the measurements. There is several existing work discussing the setup process and how to minimize such factors. One of which is covered in the work by Mancebo et al.\cite{GarciaFEETINGS} who found that there are three types of problems that occur in the domain of evaluating software's energy consumption.\cite{GarciaFEETINGS} 
\begin{enumerate}
    \item There are inconsistencies in the terminology, with different terms being used for the same concept or even the same term being used for different concepts. This lack of consistency hurts the understanding of the subject.
    \item There is no agreed-upon methodology, which makes it difficult to compare and replicate results.
    \item Choosing the correct measuring instruments that are fitting for the particular experiment evaluation.
\end{enumerate}

To solve these problems they created Framework for Energy Efficiency Testing to Improve Environmental Goals of the Software (FEETINGS). FEETINGS consist of three main components which aim to solve these issues. These are the conceptual component, methodological component and technological component.\cite{GarciaFEETINGS}

The conceptual component provides an extension of the work by García et al.\cite{GarciaSMO} which defines Software Measurement Ontology. This extension is called Green Software Measurement Ontology (GSMO) and provides terminology and the corresponding definitions to help authors of papers about energy measurements describe their process with a specified terminology. This helps readers of such papers understand and compare results from different papers. The Methodological component is a process called Green Software Measurement Process (GSMP) which are guidelines to assist with the study design, analysis and presenting the results. It is presented in seven phases, described as follows by Mancebo et al\cite*{GarciaFEETINGS}
\begin{enumerate}
    \item \textit{"Scope Definition"}: A requirements specification for evaluation of the results is made and the test cases are chosen.
    \item \textit{"Measurement Environment Setting"}: The DUT is chosen as well as the measuring instruments. The baseline measurements should also be acquired in this phase.
    \item \textit{"Measurement Environment Preparation"}: Preparation of environment
    \item \textit{"Perform the measurements"}: Experiment is conducted and data is recorded. 
    \item \textit{"Test Case Data analysis"}: The data is processed and analyzed. Here outliers can be found and a sanity check of the results is done.
    \item \textit{"Software Entity Data analysis"}: Conclusions about the experiments are started from the analysis in the previous step.
    \item \textit{"Reporting the results"}: Here the process of the study is documented, as well as the results. 
\end{enumerate} 

Mancebo et al.\cite*{GarciaFEETINGS} claim that the methodological component aid in giving more reliable and consistent measurements which in turn makes them more comparable and replicable. 

The technological component consists of two parts. The first part is the Energy Efficiency Tester (EET) which is a hardware-based approach to measuring energy consumption. It is presented in more detail by Mancebo et al. in \cite*{GarciaEET}. EET is a portable measuring instrument, which has three main components: A system microcontroller which is a Mega Arduino Development Board that is responsible for getting the data. Then there are a set of 9 sensors for measuring the energy consumption of different components of the DUT and the temperature. Lastly, a power supply, has to be used instead of the DUT's original power supply because the sensors are connected to the power supply.\cite*{GarciaEET} The second part is called ELLIOT, which is a software-based tool which handles the data provided by EET with the main goal of providing a visual representation that can be used to process, analyze and construct graphs and tables of the data collected by the EET. It can identify potential outliers and calculate different statistical variables.\cite{GarciaFEETINGS} However it should be noted that to the extent of our knowledge the ELLIOT tool is not available to the public and can therefore not be used.\newline

 Another work is by Sestoft\cite[]{sestoft2013microbenchmarks}, where a framework to measure the execution time of microbenchmarks is created in an iterative manner where different pitfalls are uncovered. The reason why energy consumption measurements are not straightforward is because of a large number of factors which has been introduced in complex modern systems. This is especially clear on managed execution platforms like the Common Language Infrastructure e.g .Net from Microsoft or the Java Virtual Machine (JVM) where software in an intermediate form is compiled to real machine code at runtime by just-in-time (JIT) compilation, which affects the execution time for a couple  reasons. One thing is the start-up overhead of the JIT or the adaptive optimization of JIT compilation. What the optimization does is to locate code executed only a few times, and code executed many times, which results in a prioritization, where a lot of time is used to generate optimized code for code executed many times and quickly generated code is made when the code is executed only a few times. The JIT compiler also avoids using a lot of time on code analysis, which can result in cases where the generated code works well in simpler contexts, and in more complex contexts, the performance is not as good. Another factor the programmer cannot control is automatic memory management, which may decide to run the garbage collection during the run of a test case, resulting in unreliable results. The same can be said for the operating system, processors and memory management systems.

In the process of creating the framework for measuring execution time, Sestoft\cite*[]{sestoft2013microbenchmarks} uses a \texttt{multiply} method which performs 20 floating point multiplications, an integer bitwise "and" and a conversion from a 32-bit int to a 64-bit double. During the first phases, one observation is that when running the test cases for many iterations, the execution time drops to zero. This happens because the JIT compiler observes that the result of the method is not used for anything, resulting in it skipping the method entirely. When measuring the runtime of the test case, only measuring one execution is deemed too little as the results vary too much. This should rather be multiple runs, and then looking at the average runtime and the standard deviation. These multiple runs are in the work by Sestoft et al.\cite*{sestoft2013microbenchmarks} deemed to be until the execution time exceeds $0.25$ seconds, as this is long enough to avoid problems with virtual machine startup and clock resolution. In the end, some additional pitfalls are noted, including:\cite*{sestoft2013microbenchmarks}

\begin{itemize}
    \item Shut down as many background services as possible, as this can impact performance
    \item The generation of logging and debug messages can use more than 90\% of execution time, so this has to be disabled
    \item An IDE uses a lot of CPU time and causes slower execution time because of debugging code, so do not execute through IDEs
    \item Disable power saving schemes so the CPU does not reduce its speed during benchmark runs.
    \item Compile with relevant optimization options so the generated bytecode does not include code to accommodate debugging
    \item Different implementations of .NET and JVM have different characteristics and different garbage collectors
    \item Different CPU brands, versions (like desktop or mobile) and hardware (like ram) have different characteristics
    \item Reflect on results, and if they look slow/fast something might have been overlooked.
\end{itemize}

%% benchmarking c# for energy consumption ørsted nielsen


Bokhari et al\cite{Bokhari2020r3} found that when running benchmarks comparing different variants of the same program on Android systems, noise had an impact on the results. This was due to noise coming from several uncontrollable factors such as:\cite*{Bokhari2020r3}

\begin{itemize}
    \item System software states changing from restart to restart and during an experiment. This is party because not all background processes from the system begin immediately after a reboot and additionally some are triggered by battery levels.
    \item Memory consumption of the Android system due to background processes that cannot be disabled and the Android memory management system.
    \item The battery voltage. Even though the system was fully charged at the starting point the voltage in the beginning and in the end of an experiment varied from run to run.
\end{itemize}

To solve this they propose a method called \textit{R3-VALIDATION}\cite*{Bokhari2020r3} which is a rotated round-robin approach to running the test cases (program variants), which ensures more fair execution conditions. In this approach the test cases (A, B, C) are rotated as follows: setup, ABC, ABC, setup, BCA, BCA, setup, CAB, CAB. Where the setup phase is a restart, initialization and recharge of the system. The amount of times to run a set of test cases between setup phases is chosen such that the battery does not deplete to a level which would cause a lot of noise be starting energy saving mechanisms. They achieved more consistent system states and memory consumption from this approach and therefor gave more fair results for the different program variants.\cite*{Bokhari2020r3}\nytafsnit

When comparing the results gathered from either different DUTs or different measuring instruments different steps needs to be taken. In the work by Dongarra et al.\cite*[]{Dongarra2012} the sampling rate is set to the same across all system, to make it more comparable.\todo[]{Extend on this}