% When running the experiments, what code is best to test a language

\section{Experimental Setup}\label[section]{sec:experimental_setup}

When running experiments on the energy consumption of hardware, it is not straightforward, as many factors can impact the measurements. There is several existing work discussing the setup process. One of these is covered in the work by Sestoft\cite[]{sestoft2013microbenchmarks}, where a framework to measure the execution time of microbenchmarks is created in an iterative manner where different pitfalls are uncovered. The reason why energy consumption measurements are not straightforward is because of a large number of factors which has been introduced in complex modern systems. This is especially clear on managed execution platforms like the Common Language Infrastructure e.g .Net from Microsoft or the Java Virtual Machine (JVM) where software in an intermediate form is compiled to real machine code at runtime by just-in-time (JIT) compilation, which affects the execution time for a couple  reasons. One thing is the start-up overhead of the JIT or the adaptive optimization of JIT compilation. What the optimization does is locate code executed only a few times, and code executed a lot, which results in a prioritization, where a lot of time is used to generate optimized code for code executed many times and quickly generated code is made when the code is executed only a few times. The JIT compiler also avoids using a lot of time on code analysis, which can result in cases where the generated code works well in simpler contexts, and in more complex contexts, the performance is not as good. Another factor the programmer cannot control is automatic memory management, which may decide to run the garbage collection during the experiments, resulting in unreliable results. The same can be said for the operating system, processors and memory management systems.

In the process of creating the framework for measuring execution time, Sestoft\cite*[]{sestoft2013microbenchmarks} uses a \texttt{multiply} method which performs 20 floating point multiplications, an integer bitwise "and" and a conversion from a 32-bit int to a 64-bit double. During the first phases, one observation is that when running the experiments for many iterations, the execution time drops to zero. This happens because the JIT compiler observes the result of the method is not used for anything, resulting in it skipping the method entirely. When measuring the runtime of the experiment, only measuring one execution is deemed too little as the results vary too much. This should rather be multiple runs, and then looking at the average runtime and the standard deviation. These multiple runs are in this study deemed to be until the execution time exceeds $0.25$ seconds, as this is long enough to avoid problems with virtual machine startup and clock resolution. In the end, some additional pitfalls are noted, including:\cite*{sestoft2013microbenchmarks}

\begin{itemize}
    \item Shut down as many background services as possible, as this can impact performance
    \item The generation of logging and debug messages can use more than 90\% of execution time, so this should be disabled
    \item An IDE uses a lot of CPU time and causes slower execution time because of debugging code, so do not execute through IDEs
    \item Disable power saving schemes so the CPU does not reduce its speed during benchmark runs.
    \item Compile with relevant optimization options so the generated bytecode does not include code to accommodate debugging
    \item Different implementations of .NET and JVM have different characteristics and different garbage collectors
    \item Different CPU brands, versions (like desktop or mobile) and hardware (like ram) have different characteristics
    \item Reflect on results, and if they look slow/fast something might have been overlooked.
\end{itemize}

%% benchmarking c# for energy consumption Ã¸rsted nielsen


Bokhari et al\cite{Bokhari2020r3} found that when running benchmarks comparing different variants of the same program on Android systems, noise had an impact on the results. This was due to noise coming from several uncontrollable factors:\cite*{Bokhari2020r3}
\begin{itemize}
    \item Background processes can not be fully controlled during the execution of the experiments
    \item Memory consumption of the Android system and background processes
    \item The battery voltage. Even though the system was fully charged at the starting point the voltage in the beginning and in the end of an experiment varied from run to run.
\end{itemize}
To solve this they propose a method called \textit{R3-VALIDATION} which is a rotated round-robin approach to running the program variants, which ensures more fair execution conditions. In this approach the variants (A, B, C) are rotated as follows: setup, ABC, ABC, setup, BCA, BCA, setup, CAB, CAB. Where the setup phase is a restart, initialization and recharge of the system. They achieved more consistent system states from this approach. \cite*{Bokhari2020r3}\nytafsnit

When comparing the results gathered from either different systems or different measurement tool,s different steps needs to be taken. In the work by Dongarra et al.\cite*[]{Dongarra2012} the sampling rate is set to the same across all system, to make it more comparable.\todo[]{Extend on this}