\subsection{Distribution}
Here the method for statistically comparing the distribution of the different measurement will be elaborated and explained.

\paragraph{Shapiro-Wilk}
One of the most common preconditions for statistical tests is that the data should be a normal distribution for the results to work optimally. To test if a distribution is a normal Distribution a normalcy test can be used. While there are many different normalcy tests the Shapiro-Wilk test seems generally more powerful than the others that are commonly used\cite{razali2011power}. Because of this the Shapiro-Wilk test will be used to check for normal distributions. The formula for the Shapiro-Wilk test is:

\begin{equation}
    W=\frac{( \sum{a_i x_i} )^2}{\sum{(x_i - \bar{x})^2}}
\end{equation}

The null hypnosis $H_0$ for the test is:

\begin{quotation}
    \textit{The sample comes from a normally-distributed population}
\end{quotation}

\todo[]{Confidence level the same in both cochran and stats}
To find whether $H_0$ can be rejected, the $p_{value}$ is used. The $p_{Value}$ or probability value is the probability of achieving the same results under $H_0$ of a statistical test. Alpha represents the statistical significance, a common threshold is $0.05$. If the $p_{Value}$ is less than alpha, it can reject $H_0$. Using the $p_{value}$ is commonplace in statistical testing[greenland2016statistical], and it is also used for the rest of the tests in this section and unless specified otherwise all test are conducted with an alpha of $0.05$.

\paragraph{T-Test}

If the $H_0$ for the Shapiro-Wilk test could not be rejected it is assumed that the distribution is normal or close enough not to be significant.

In the case of a normal distribution a students t-test can be utilized to check if there is a statistically significant difference between the two samples. There are a couple of conditions when using t-test these are:

\begin{itemize}
    \item Both samples should follow a normal Distribution.
    \item The samples are independent of each other.
    \item Both the samples should also have approximately the same variance.
\end{itemize}

The null hypothesis $H_0$ for the t-test is as follows:

\begin{quote}
    \textit{There is no statistically significant difference between the samples}    
\end{quote}

The t-test is performed using two values the $t_{value}$ and the $p_{value}$ and the formula for the calculation of the $t$ value can be seen in:

\begin{equation}
    t_{Value} = \frac{|\bar{x_1}- \bar{x_2}|}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}  
\end{equation}

\paragraph{Mann-Whitney U test}
If however it is determined that the distributions are not normally distributed then another test will have to be made instead of the student t-test. A popular option for this is the Mann-Whitney U test. There are some important differences between the two methods. The t-test calculates the difference in the mean between two samples, while the Mann-Whitney U test checks if there is a difference in the rank sum of the samples\cite{mann1947test}. 

The idea behind the rank sum is that each datapoint is given a rank, this rank is based one its position in a sorted order between all of the other datapoints. 
The basic requirements for using the Mann-Whitney U test is to have ordinal variables, this just means that there is a way to order and sort them.

The null hypothesis $H_0$ for the Mann-Whitney U test is defined as:

\begin{quote}
    \textit{In the population, the sum of the rankings in the two group's does not differ.}
\end{quote}

Doing the calculations require a series of calculations where the initial is finding $U_1$ and $U_2$.

The actual calculations for each group consist of the number of cases in the group $n_1$ and rank sum $T_1$. These are then used in the formula:

\begin{equation}
    U_1 = n_1*n_2+\frac{n_1*(n_1+1)}{2}-T_1  
\end{equation}

\begin{equation}
    U_2 = n_1*n_2+\frac{n_2*(n_2+1)}{2}-T_2
\end{equation}

The smallest of these two will then be used in the test as $U=min(U_1,U_2)$. The next calculations cover the calculations of $\varphi U$ and $\sigma U$. $\varphi U$ is the expected value of $U$, while $\sigma U$ is the standard error of $U$.

\begin{equation}
    \varphi U = \frac{n_1*n_2}{2}
\end{equation}

\begin{equation}
    \sigma U = \sqrt{\frac{n_1*n_2*(n_1+n_2+1)}{12}}
\end{equation}

Then to calculate the value $z$.

\begin{equation}
    z = \frac{U-\varphi U}{\sigma U}
\end{equation}

using z to find the p-value to check if $H_0$ can be rejected, again the alpha is set to $0.05$. 

\paragraph{Kolmogorov-Smirnov test}
Comes in two different variations the one sample and two sample Kolmogorov Smirnov test. The one sample test is also known as the Kolmogorov Smirnov normalcy test(KS1) it test how close to the normal distribution a sample is, much like the Shapiro-wilk test. The two sample Kolmogorov Smirnov test(kS2) is however a bit different as instead of checking if a sample is normally distributed it instead check if the samples is from the same population as some other provided sample. Thus KS2 is used here to determine if the two samples are from the same population. To calculate the KS2 the following series of calculations are needed.
$$D_{n,m} = max(|F_1(x)-F_2(x)|)$$
where:
$$F_1 = cdf(sample1,x)$$
$$F_2 = cdf(sample2,x)$$
where cdf (cumulative distribution function) assuming a sorted list of observations counts the number of observations below x and the divide with the total number of samples. The KS2 depends on a parameter $en$, which can be calculated using the following formula:
$$en=\frac{(m*n)}{(m+n)}$$
where $n$ is the size of sample 1 and $m$ is the size of sample 2.
The $H_0$ for KS2: \textit{The two samples are from the same distribution}.
$H_0$ can be rejected in the case of $D_{n,m} > c(\alpha)\sqrt{en}$\cite{massey1951kolmogorov}
