\subsection{How to Measure}\label{subsec:how_to_measure}

When running the experiments, different types of measuring approaches will be used, based on the tool represented in \cref{sec:hardware}. The first approach is system-level physical measurements, more specifically AC measurements, which was found in \cref{sec:rw_diff_measuring_instruments} to be the most accurate way to measure the energy consumption of the DUT. This approach will serve as the ground truth for the experiments conducted.

Another approach is the model-based interface, where on-chip sensors are used. For this, \cref{sec:rw_diff_measuring_instruments} found that RAPL is a frequently used version of this. RAPL however only works on Linux, as was covered in \cref{subsec:rapl}, and since this work aims to find the accuracy of Windows approaches, some additional energy profilers were introduced in \cref{sec:measuring_instruments}. RAPL is however still included, as it also serves as a good benchmark in terms of what is possible when considering only software measurements. When considering the Windows energy profilers, the LHM and Intel Power Gadget are very similar to both RAPL and the physical measurements, as they find the total energy consumption of the entire system. Here E3 is different, as this profiler can estimate the energy consumption on a process-to-process level, and also claims to be able to benefit from hardware chips, if the DUT is equipped with such one.\cite*{E3WinHec} E3 however also comes with some uncertainty, which is covered in \cref{sec:E3Experiments}.

Given RAPL, Intel Power Gadget, LHM and the physical measurements only provide the energy consumption of the entire system, the idle energy consumption is required for this experiment, to extract the energy consumption of the test program only. To solve this, dynamic energy consumption can be utilized, as presented by Fahad et al.\cite{fahad2019comparative}, which was covered in \cref{sec:rw_measureing_methodology_setup} Dynamic energy consumption is an estimation of the applications power consumption and is calculated using \cref{eq:dynamicEnergy}
%.\todo[]{Dynamic energy is now part of the related work, might remove or reference it here} 

%\begin{equation}
%    E_D = E_T -(P_S * T_E)
%\end{equation}

%Where $E_D$ is the dynamic energy consumption, $E_T$ is the total energy consumption by the DUT when running the experiment, $T_E$ is the duration of the program execution and $P_S$ is the energy consumption when the system is idle. The dynamic energy consumption will then represent the energy consumption of the test case. 
To do this a certain amount of control over the system is required and some precautions are necessary. The procedures that will be used are different for the DUTs and OSes, but some will be the same for all DUTs as shown here, and are a combination of procedures presented in \cref{sec:rw_measureing_methodology_setup} by Sestoft\cite*[]{sestoft2013microbenchmarks} and Fahad et al.\cite*[]{fahad2019comparative}:

\begin{itemize}
    \item The machines are reserved exclusively for the experiments, this is to prevent divinations in the results.
    \item The networking will be disabled on the machines to ensure that these do not affect the results.
    \item The processes and the temperature of the machines will be measured before and after each experiment.
    \item After each batch, the DUT will be restarted and its setup phase is performed before continuing to the next batch.
    \item The memory of the Test cases will not exceed the DUT's main memory to avoid memory swapping
    \item Some baseline power usage will be established by measurements before and after the experiments
    \item The generation of logging and debug messages can use more than 90\% of execution time, so this has to be disabled
    \item Disable power-saving schemes so the CPU does not reduce its speed during benchmark runs.
    \item Reflect on results, and if they look slow/fast something might have been overlooked.
\end{itemize}

Each batch will be tested on every DUT with each type of OS these being Windows 10 and Ubuntu. Each DUT will have a slightly different setup depending on the hardware and the OS of the system. For the Windows and Linux machines, the various unnecessary background processes will be disabled to ensure as little noise in the measurements as possible\cite*[]{sestoft2013microbenchmarks}, the specific process disabled can be seen in \cref{tab:disabled_proc}.\todo{How did we decide this} There are no disabled processes on the Linux OS, this was choice was made on the general assumption that Ubuntu has less unnecessary background processes than Windows.\todo{Does a proper source exist for this statement}. A difference between the DUTs is if whether they are desktops or laptops. On the desktop it is possible to perform system-level physical measurements because the measurement is system-wide it is important to have as close to zero non test case related power fluctuation as possible, this is why the fans of the system will be set to max during all the measurement times. For the laptops, this is not an issue as the measurements do not include the whole system. For the laptops, some special controls are necessary for the measurements because measurement quality can become worse if the laptops are connected to the charger during the test case runs, since then E3 will not use the Maxim chip\cite{E3Video}. To prevent this the charger is shut off during the experiments, and then turned on to fully recharge the DUT during the setup phase, this cycle will repeat until all of the experiments are done.

\input{tabels/disabled_Processes.tex}

\subsection{What to Measure}\label[]{subsec:test_cases}

Another thing to consider is which test programs make sense to run during the experiments. Based on what was found in \cref{ch:related_work}, two common repositories are used, either Rosetta Code\cite*[]{rosetta_code} or the Computer Language Benchmark Game\cite*[]{benchmark_game}. Here different benchmarks are chosen with different sizes and loads on different parts of the DUT. In the end, the choices were made based on the work by Lima et al.\cite*[]{greenland2016statistical} and Koedijk et al.\cite*[]{Koedijk2022diff}, where the benchmarks tested either the CPU, memory, synchronization, IO or disk. The choices for the experiments conducted in this work can be seen in \cref{tab:benchmarks}.

\input{tabels/benchmarks.tex}


